{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercises here presented try to provide some practical introduction to general and useful data handling steps.\n",
    "\n",
    "The intention is to practice and become more comfortable with tools commonly used during such processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generated sample data\n",
    "2. Glucose data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported/required packages:\n",
    "\n",
    "- [numpy](numpy.org)\n",
    "- [pandas](pandas.pydata.org)\n",
    "- [matplotlib](matplotlib.org)\n",
    "- [scipy](scipy.org)\n",
    "\n",
    "__Note:__ make sure to have a look at the used/required packages website including their documentation and resources. These libraries are heavily used when dealing with data in a python environment.\n",
    "\n",
    "For any extra -- and more general -- information see: \n",
    "- [Book: Python for Everybody](py4e.com/book.php)\n",
    "- [Book: Python Data Science Handbook](jakevdp.github.io/PythonDataScienceHandbook/)\n",
    "- [Tutorials, Exercises](pynative.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# requirements list\n",
    "dependencies = [\n",
    "  'pandas',\n",
    "  'scipy',\n",
    "  'matplotlib',\n",
    "]\n",
    "\n",
    "# requirements check\n",
    "pkg_resources.require(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#plt.style.use('seaborn')\n",
    "plt_fig_size = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generated Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can come in different flavors and structures. For now, let us generate our own data. The following exercise makes use of data generated from an arbitrary function.\n",
    "\n",
    "**EXERCISE** \n",
    "<br>\n",
    "_Use the function $y = x * sin(x^2) + 1$ to generate an output of $100$ data points with $-2 \\leq x \\leq 3$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function: (y = x * sin(x^2) + 1 | dy/dx = sin(x^2) + 2x^2 * cos(x^2)) -> en.wikipedia.org/wiki/Derivative\n",
    "x = \n",
    "y = \n",
    "\n",
    "# data\n",
    "data = np.column_stack((x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is used as input for functions that expect it in specific formats. Usually array/matrix based format is the way to go, which leads to the extensive usage of **Numpy**'s [n-dimensional arrays](numpy.org/doc/stable/reference/arrays.ndarray.html).\n",
    "\n",
    "**EXERCISE** \n",
    "<br>\n",
    "_Output the $10$ first entries of `data`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy n-dimensional array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the visualization of array/matrix formatted data is not as good as table-like format. With this, __pandas__'s [_DataFrame_](pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe) can be used to store - and also output - data in a more readable structure.\n",
    "\n",
    "**EXERCISE** \n",
    "<br>\n",
    "Using `data`:\n",
    "1. _Create a _DataFrame_ and store it in a variable called `df`._\n",
    "2. _Name its columns as `x` and `y`._\n",
    "3. _Display the first $10$ entries of the _DataFrame_._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = \n",
    "\n",
    "# display the first 10 entries of df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE** \n",
    "<br>\n",
    "_Plot the data contained in the _DataFrame_._\n",
    "\n",
    "__Tip:__ the entire raw data using function outputs as the `y` values and data point indices as the `x`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib to plot here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data can come with a high degree of variation and/or noise. To make it more smooth, which can facilitate visualization, some filters can be applied.\n",
    "\n",
    "In this exercise, [_rolling_](pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) median and [_Gaussian_](docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.gaussian_filter1d.html) will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian and rolling window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE** \n",
    "<br>\n",
    "_Add to the already existing `df` _DataFrame_ two more columns for each filter (_rolling median_, and _Gaussian_)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "df['filtered_median'] = \n",
    "df['filtered_gaussian'] = \n",
    "\n",
    "# display df (e.g., first 10 entries) with the new column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ other aggregation such as _mean_ can be used when applying `rolling()`, so make sure to look for different ones and try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE** \n",
    "<br>\n",
    "_Plot the previous filtered values together with the raw data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both filters can be tweaked by, for instance, varying windows size for rolling and/or using mean instead of median, and using a different $\\sigma$ value for _Gaussian_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tools to be used here can serve us to identify special values and variations within the data. For that purpose let us start with __pandas__ [`diff()`](pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html).\n",
    "<br>\n",
    "This method evaluated the distance (differences) from consecutive values in a Series, or in the entire DataFrame.\n",
    "\n",
    "**EXERCISE** \n",
    "<br>\n",
    "_Add a new column to `df` called `y_diff` that has the output of `diff()` applied to `y`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_diff'] = \n",
    "\n",
    "# display df with the new column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ the very first value of the new added column `y_diff` is set as `NaN`. This is because __pandas__ cannot evaluate the difference for the first value as there is no previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "<br>\n",
    "_By using the values in the previous added column, answer: which data point is associated to the largest decrease and increase when coming from the previous data point?_\n",
    "_For that:_\n",
    "<br>\n",
    "1. _Get the index of the entry with the lowest value of `y_diff`._\n",
    "2. _Get the index of the entry with the highest value of `y_diff`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest value index\n",
    "idx_min_diff = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest value index\n",
    "idx_max_diff = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "<br>\n",
    "_Now, show the values of both lowest and highest differences._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using derivatives for trending identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous approach using `diff()` might be enough to get aware of how data varies over time (increasing, decreasing). This can be seen when plotting the data.\n",
    "\n",
    "**EXERCISE**\n",
    "<br>\n",
    "_Plot both `y` and `y_diff` together._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, depending on how volatile the data are, such identification becomes more difficult. For that, approximated derivatives come as a tool for such identification.\n",
    "\n",
    "**EXERCISE**\n",
    "<br>\n",
    "_Calculate such approximation:_\n",
    "\n",
    "\n",
    "1. _Using the already evaluated differences._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximated derivative\n",
    "df['deriv'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. _Using __Numpy__ [`gradient()`](Numpy.org/doc/stable/reference/generated/numpy.gradient.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximated derivative\n",
    "df['deriv_gradient'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. _Appying the already mentioned _Gaussian_ filter, but now using `order=1`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aproximate derivative\n",
    "df['deriv_filtered_gaussian'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__: the `order` parameter sets the derivative order used for the filter approximation, which means that by using a first order filter (`order=1`) we are getting an approximation to the 1st derivative. The default value is `order=0`, and can be seeing as an approximation to the original function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "<br>\n",
    "_Plot the approximated derivative values, each one in a subplot._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Glucose Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next exercises, real - and already collected - data will be used. The values are related to Blood Glucose, and were collected for one person during some days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "The data is distributed in a `.xml` file containing specific events as nodes. This is a very declarative/container type structure, and the one to be used here contains specific observations that will compose the data to be handled: events including time and blood glucose values.\n",
    "\n",
    "**EXERCISE**\n",
    "<br>\n",
    "1. _Extract from the `training.xml` the nodes containing glucose events, i.e., all the `<event ts=\"...\" value=\"...\"/>` nodes._\n",
    "2. _Store it in `glucose_event_nodes`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# parse XML data\n",
    "patient_xml_root = \n",
    "glucose_event_nodes = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "With `.xml` the nodes in hands, the data must be now handled in a more easy way. For that - as already covered - let us, from this data, create a structure that contains and facilitates handling it.\n",
    "\n",
    "**EXERCISE**\n",
    "<br>\n",
    "_For that:_\n",
    "1. _From the event nodes, create a _DataFrame_ (assign it to `df`) containing columns for timestamp (name it `ts`) and blood glucose values (name it `value`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. _Display the type of the columns of `df`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] use adf information/summary display method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ When creating a _DataFrame_ coming from a more diverse data source, it is not guaranteed that __pandas__ will set the types of the columns correctly. So it is important to always check and then set the types properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. _Set columns to their proper type: `value` to numeric, and `ts` to datetime. Check types after._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value'] = \n",
    "df['ts'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display column types and or df info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a time series is a sequence of values gathered continuously through time, it makes sense to associate each data point to a moment in time instead of an integer index.\n",
    "Thus, to get the best out of __pandas__ when dealing with time series, let us set `ts` as `df`'s index.\n",
    "\n",
    "4. _Set `ts` column as the index of `df`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'ts' as the index of df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a properly set _DataFrame_ is stored, an extra step can be done to persist this for further usage.\n",
    "\n",
    "**EXERCISE**\n",
    "<br>\n",
    "_Store the data of `df` in a `.csv` file called `glucose.csv`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a .csv file to be used on further steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Persisting data in a interchangeable format as `.csv` facilitates further usage, thus is a very common part of the preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting parts of the time series (day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time series are commonly long and continuous streams of data. But very often, specific parts of it must be taken for analysis.\n",
    "\n",
    "**EXERCISE**\n",
    "<br>\n",
    "_Use indexing to isolate:_\n",
    "\n",
    "1. _The data of the 5th day in the blood glucose time series (display and plot)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. _The data of the day with the first occurrence of the highest blood glucose value in the whole time series (display and plot)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let us apply to the blood glucose time series the same filters used in the generated data from Section 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "<br>\n",
    "1. _Use the stored `glucose.csv` to reload the whole time series data into `df`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] display df information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TIP:__ Make sure to check and set the types, as well as set `ts` as the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. _Apply the previous covered filters to the whole time series (display and plot)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "df['filtered_median'] = \n",
    "df['filtered_gaussian'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. _Show the 5th day of time series including the filtered values (display and plot)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ the _Gaussian_ filter used will not get small window variations. This is due to the fact that the $\\sigma$ used cannot is not sensitive enough to catch such variations. Try using $\\sigma < 5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "<br>\n",
    "_Use the same techniques previsously mentioned (´diff()´, approximated derivatives) to identify the variations in the time series._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use diff()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using derivatives for trending identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE**\n",
    "<br>\n",
    "_Use the approximated derivatives to identify different trends._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the derivative approximations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f59018cbe3c7e2d07b0b58d4613982d89077f2209653655163822b02cab44db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
